{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51369ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AnacondaPython\\envs\\Thesis\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pykeen.sampling.negative_sampler import NegativeSampler\n",
    "from pykeen.sampling.basic_negative_sampler import BasicNegativeSampler\n",
    "from pykeen.triples.triples_factory import TriplesFactory\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdea3549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\AnacondaPython\\\\envs\\\\Thesis\\\\lib\\\\site-packages\\\\pykeen\\\\sampling\\\\negative_sampler.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "\n",
    "inspect.getfile(NegativeSampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe03d9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AnacondaPython\\envs\\Thesis\\lib\\site-packages\\pykeen\\nn\\representation.py:372: UserWarning: Directly use Embedding.max_id instead of num_embeddings.\n",
      "  warnings.warn(f\"Directly use {self.__class__.__name__}.max_id instead of num_embeddings.\")\n",
      "Training epochs on cpu:   0%|          | 0/128 [00:00<?, ?epoch/s]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   0%|          | 0/128 [00:00<?, ?epoch/s, loss=0.00656, prev_loss=nan]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▏         | 2/128 [00:00<00:11, 11.42epoch/s, loss=0.00641, prev_loss=0.00656]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   2%|▏         | 2/128 [00:00<00:11, 11.42epoch/s, loss=0.00623, prev_loss=0.00641]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   3%|▎         | 4/128 [00:00<00:10, 11.86epoch/s, loss=0.00622, prev_loss=0.00623]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   3%|▎         | 4/128 [00:00<00:10, 11.86epoch/s, loss=0.00588, prev_loss=0.00622]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   5%|▍         | 6/128 [00:00<00:10, 11.84epoch/s, loss=0.0062, prev_loss=0.00588] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   5%|▍         | 6/128 [00:00<00:10, 11.84epoch/s, loss=0.00572, prev_loss=0.0062]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▋         | 8/128 [00:00<00:09, 12.48epoch/s, loss=0.00589, prev_loss=0.00572]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   6%|▋         | 8/128 [00:00<00:09, 12.48epoch/s, loss=0.00568, prev_loss=0.00589]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▊         | 10/128 [00:00<00:09, 12.65epoch/s, loss=0.00543, prev_loss=0.00568]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   8%|▊         | 10/128 [00:00<00:09, 12.65epoch/s, loss=0.00559, prev_loss=0.00543]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   9%|▉         | 12/128 [00:00<00:09, 12.54epoch/s, loss=0.00541, prev_loss=0.00559]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   9%|▉         | 12/128 [00:01<00:09, 12.54epoch/s, loss=0.00544, prev_loss=0.00541]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  11%|█         | 14/128 [00:01<00:09, 12.37epoch/s, loss=0.00542, prev_loss=0.00544]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  11%|█         | 14/128 [00:01<00:09, 12.37epoch/s, loss=0.00533, prev_loss=0.00542]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▎        | 16/128 [00:01<00:09, 12.38epoch/s, loss=0.00517, prev_loss=0.00533]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  12%|█▎        | 16/128 [00:01<00:09, 12.38epoch/s, loss=0.00528, prev_loss=0.00517]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▍        | 18/128 [00:01<00:08, 12.30epoch/s, loss=0.00493, prev_loss=0.00528]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  14%|█▍        | 18/128 [00:01<00:08, 12.30epoch/s, loss=0.0049, prev_loss=0.00493] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▌        | 20/128 [00:01<00:08, 12.19epoch/s, loss=0.00507, prev_loss=0.0049]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  16%|█▌        | 20/128 [00:01<00:08, 12.19epoch/s, loss=0.00508, prev_loss=0.00507]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  17%|█▋        | 22/128 [00:01<00:08, 12.47epoch/s, loss=0.00482, prev_loss=0.00508]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  17%|█▋        | 22/128 [00:01<00:08, 12.47epoch/s, loss=0.00512, prev_loss=0.00482]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  19%|█▉        | 24/128 [00:01<00:08, 12.24epoch/s, loss=0.00481, prev_loss=0.00512]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  19%|█▉        | 24/128 [00:02<00:08, 12.24epoch/s, loss=0.00477, prev_loss=0.00481]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██        | 26/128 [00:02<00:08, 12.22epoch/s, loss=0.00489, prev_loss=0.00477]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  20%|██        | 26/128 [00:02<00:08, 12.22epoch/s, loss=0.0048, prev_loss=0.00489] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▏       | 28/128 [00:02<00:08, 12.21epoch/s, loss=0.0046, prev_loss=0.0048] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  22%|██▏       | 28/128 [00:02<00:08, 12.21epoch/s, loss=0.00468, prev_loss=0.0046]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  23%|██▎       | 30/128 [00:02<00:07, 12.32epoch/s, loss=0.00467, prev_loss=0.00468]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  23%|██▎       | 30/128 [00:02<00:07, 12.32epoch/s, loss=0.00468, prev_loss=0.00467]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  25%|██▌       | 32/128 [00:02<00:07, 12.44epoch/s, loss=0.00448, prev_loss=0.00468]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  25%|██▌       | 32/128 [00:02<00:07, 12.44epoch/s, loss=0.00448, prev_loss=0.00448]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  27%|██▋       | 34/128 [00:02<00:08, 11.71epoch/s, loss=0.00467, prev_loss=0.00448]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  27%|██▋       | 34/128 [00:02<00:08, 11.71epoch/s, loss=0.00426, prev_loss=0.00467]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|██▊       | 36/128 [00:02<00:07, 11.95epoch/s, loss=0.00435, prev_loss=0.00426]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  28%|██▊       | 36/128 [00:03<00:07, 11.95epoch/s, loss=0.00436, prev_loss=0.00435]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|██▉       | 38/128 [00:03<00:07, 12.18epoch/s, loss=0.00427, prev_loss=0.00436]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  30%|██▉       | 38/128 [00:03<00:07, 12.18epoch/s, loss=0.00419, prev_loss=0.00427]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  31%|███▏      | 40/128 [00:03<00:07, 12.31epoch/s, loss=0.00422, prev_loss=0.00419]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  31%|███▏      | 40/128 [00:03<00:07, 12.31epoch/s, loss=0.00436, prev_loss=0.00422]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  33%|███▎      | 42/128 [00:03<00:06, 12.37epoch/s, loss=0.00423, prev_loss=0.00436]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  33%|███▎      | 42/128 [00:03<00:06, 12.37epoch/s, loss=0.00425, prev_loss=0.00423]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  34%|███▍      | 44/128 [00:03<00:06, 12.57epoch/s, loss=0.00444, prev_loss=0.00425]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:  34%|███▍      | 44/128 [00:03<00:06, 12.57epoch/s, loss=0.00424, prev_loss=0.00444]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|███▌      | 46/128 [00:03<00:06, 12.47epoch/s, loss=0.00419, prev_loss=0.00424]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  36%|███▌      | 46/128 [00:03<00:06, 12.47epoch/s, loss=0.00418, prev_loss=0.00419]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|███▊      | 48/128 [00:03<00:06, 12.38epoch/s, loss=0.00408, prev_loss=0.00418]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  38%|███▊      | 48/128 [00:03<00:06, 12.38epoch/s, loss=0.00408, prev_loss=0.00408]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  39%|███▉      | 50/128 [00:04<00:06, 12.03epoch/s, loss=0.0042, prev_loss=0.00408] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  39%|███▉      | 50/128 [00:04<00:06, 12.03epoch/s, loss=0.00394, prev_loss=0.0042]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  41%|████      | 52/128 [00:04<00:06, 12.19epoch/s, loss=0.00419, prev_loss=0.00394]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  41%|████      | 52/128 [00:04<00:06, 12.19epoch/s, loss=0.00416, prev_loss=0.00419]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▏     | 54/128 [00:04<00:06, 12.12epoch/s, loss=0.00409, prev_loss=0.00416]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  42%|████▏     | 54/128 [00:04<00:06, 12.12epoch/s, loss=0.00393, prev_loss=0.00409]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▍     | 56/128 [00:04<00:05, 12.12epoch/s, loss=0.00402, prev_loss=0.00393]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  44%|████▍     | 56/128 [00:04<00:05, 12.12epoch/s, loss=0.00397, prev_loss=0.00402]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  45%|████▌     | 58/128 [00:04<00:05, 12.36epoch/s, loss=0.00412, prev_loss=0.00397]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  45%|████▌     | 58/128 [00:04<00:05, 12.36epoch/s, loss=0.00384, prev_loss=0.00412]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  47%|████▋     | 60/128 [00:04<00:05, 12.73epoch/s, loss=0.00391, prev_loss=0.00384]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  47%|████▋     | 60/128 [00:04<00:05, 12.73epoch/s, loss=0.00379, prev_loss=0.00391]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|████▊     | 62/128 [00:05<00:05, 12.26epoch/s, loss=0.00386, prev_loss=0.00379]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  48%|████▊     | 62/128 [00:05<00:05, 12.26epoch/s, loss=0.00381, prev_loss=0.00386]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████     | 64/128 [00:05<00:05, 12.15epoch/s, loss=0.00407, prev_loss=0.00381]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████     | 64/128 [00:05<00:05, 12.15epoch/s, loss=0.00392, prev_loss=0.00407]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▏    | 66/128 [00:05<00:05, 12.27epoch/s, loss=0.0038, prev_loss=0.00392] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  52%|█████▏    | 66/128 [00:05<00:05, 12.27epoch/s, loss=0.00387, prev_loss=0.0038]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  53%|█████▎    | 68/128 [00:05<00:04, 12.34epoch/s, loss=0.00377, prev_loss=0.00387]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  53%|█████▎    | 68/128 [00:05<00:04, 12.34epoch/s, loss=0.00382, prev_loss=0.00377]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|█████▍    | 70/128 [00:05<00:04, 12.45epoch/s, loss=0.00385, prev_loss=0.00382]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  55%|█████▍    | 70/128 [00:05<00:04, 12.45epoch/s, loss=0.00366, prev_loss=0.00385]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|█████▋    | 72/128 [00:05<00:04, 12.68epoch/s, loss=0.00362, prev_loss=0.00366]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  56%|█████▋    | 72/128 [00:05<00:04, 12.68epoch/s, loss=0.00373, prev_loss=0.00362]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|█████▊    | 74/128 [00:06<00:04, 12.57epoch/s, loss=0.00349, prev_loss=0.00373]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  58%|█████▊    | 74/128 [00:06<00:04, 12.57epoch/s, loss=0.00372, prev_loss=0.00349]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  59%|█████▉    | 76/128 [00:06<00:04, 12.74epoch/s, loss=0.00363, prev_loss=0.00372]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  59%|█████▉    | 76/128 [00:06<00:04, 12.74epoch/s, loss=0.00368, prev_loss=0.00363]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  61%|██████    | 78/128 [00:06<00:03, 12.88epoch/s, loss=0.00355, prev_loss=0.00368]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  61%|██████    | 78/128 [00:06<00:03, 12.88epoch/s, loss=0.00355, prev_loss=0.00355]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|██████▎   | 80/128 [00:06<00:03, 13.06epoch/s, loss=0.00355, prev_loss=0.00355]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  62%|██████▎   | 80/128 [00:06<00:03, 13.06epoch/s, loss=0.00358, prev_loss=0.00355]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|██████▍   | 82/128 [00:06<00:03, 12.86epoch/s, loss=0.00357, prev_loss=0.00358]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  64%|██████▍   | 82/128 [00:06<00:03, 12.86epoch/s, loss=0.00361, prev_loss=0.00357]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|██████▌   | 84/128 [00:06<00:03, 12.99epoch/s, loss=0.00362, prev_loss=0.00361]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  66%|██████▌   | 84/128 [00:06<00:03, 12.99epoch/s, loss=0.00358, prev_loss=0.00362]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  67%|██████▋   | 86/128 [00:06<00:03, 13.17epoch/s, loss=0.00345, prev_loss=0.00358]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  67%|██████▋   | 86/128 [00:06<00:03, 13.17epoch/s, loss=0.00345, prev_loss=0.00345]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  69%|██████▉   | 88/128 [00:07<00:03, 13.24epoch/s, loss=0.00349, prev_loss=0.00345]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  69%|██████▉   | 88/128 [00:07<00:03, 13.24epoch/s, loss=0.00357, prev_loss=0.00349]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  70%|███████   | 90/128 [00:07<00:02, 13.56epoch/s, loss=0.00349, prev_loss=0.00357]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:  70%|███████   | 90/128 [00:07<00:02, 13.56epoch/s, loss=0.00346, prev_loss=0.00349]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|███████▏  | 92/128 [00:07<00:02, 13.43epoch/s, loss=0.00344, prev_loss=0.00346]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  72%|███████▏  | 92/128 [00:07<00:02, 13.43epoch/s, loss=0.00351, prev_loss=0.00344]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  73%|███████▎  | 94/128 [00:07<00:02, 13.37epoch/s, loss=0.00339, prev_loss=0.00351]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  73%|███████▎  | 94/128 [00:07<00:02, 13.37epoch/s, loss=0.0035, prev_loss=0.00339] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  75%|███████▌  | 96/128 [00:07<00:02, 13.02epoch/s, loss=0.00342, prev_loss=0.0035]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  75%|███████▌  | 96/128 [00:07<00:02, 13.02epoch/s, loss=0.00352, prev_loss=0.00342]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  77%|███████▋  | 98/128 [00:07<00:02, 13.13epoch/s, loss=0.00354, prev_loss=0.00352]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  77%|███████▋  | 98/128 [00:07<00:02, 13.13epoch/s, loss=0.00347, prev_loss=0.00354]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|███████▊  | 100/128 [00:07<00:02, 13.24epoch/s, loss=0.00345, prev_loss=0.00347]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  78%|███████▊  | 100/128 [00:08<00:02, 13.24epoch/s, loss=0.00332, prev_loss=0.00345]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|███████▉  | 102/128 [00:08<00:01, 13.16epoch/s, loss=0.00338, prev_loss=0.00332]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  80%|███████▉  | 102/128 [00:08<00:01, 13.16epoch/s, loss=0.00345, prev_loss=0.00338]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  81%|████████▏ | 104/128 [00:08<00:01, 12.95epoch/s, loss=0.00332, prev_loss=0.00345]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  81%|████████▏ | 104/128 [00:08<00:01, 12.95epoch/s, loss=0.00329, prev_loss=0.00332]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  83%|████████▎ | 106/128 [00:08<00:01, 12.96epoch/s, loss=0.0035, prev_loss=0.00329] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  83%|████████▎ | 106/128 [00:08<00:01, 12.96epoch/s, loss=0.00309, prev_loss=0.0035]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|████████▍ | 108/128 [00:08<00:01, 12.86epoch/s, loss=0.00339, prev_loss=0.00309]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  84%|████████▍ | 108/128 [00:08<00:01, 12.86epoch/s, loss=0.00331, prev_loss=0.00339]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|████████▌ | 110/128 [00:08<00:01, 12.95epoch/s, loss=0.00324, prev_loss=0.00331]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  86%|████████▌ | 110/128 [00:08<00:01, 12.95epoch/s, loss=0.00332, prev_loss=0.00324]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|████████▊ | 112/128 [00:08<00:01, 13.21epoch/s, loss=0.00313, prev_loss=0.00332]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  88%|████████▊ | 112/128 [00:08<00:01, 13.21epoch/s, loss=0.00325, prev_loss=0.00313]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  89%|████████▉ | 114/128 [00:09<00:01, 13.06epoch/s, loss=0.00342, prev_loss=0.00325]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  89%|████████▉ | 114/128 [00:09<00:01, 13.06epoch/s, loss=0.0033, prev_loss=0.00342] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  91%|█████████ | 116/128 [00:09<00:00, 13.09epoch/s, loss=0.00331, prev_loss=0.0033]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  91%|█████████ | 116/128 [00:09<00:00, 13.09epoch/s, loss=0.00334, prev_loss=0.00331]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|█████████▏| 118/128 [00:09<00:00, 13.03epoch/s, loss=0.00323, prev_loss=0.00334]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  92%|█████████▏| 118/128 [00:09<00:00, 13.03epoch/s, loss=0.00319, prev_loss=0.00323]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|█████████▍| 120/128 [00:09<00:00, 12.86epoch/s, loss=0.00311, prev_loss=0.00319]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  94%|█████████▍| 120/128 [00:09<00:00, 12.86epoch/s, loss=0.00331, prev_loss=0.00311]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  95%|█████████▌| 122/128 [00:09<00:00, 12.97epoch/s, loss=0.00312, prev_loss=0.00331]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  95%|█████████▌| 122/128 [00:09<00:00, 12.97epoch/s, loss=0.0032, prev_loss=0.00312] \n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  97%|█████████▋| 124/128 [00:09<00:00, 13.07epoch/s, loss=0.00308, prev_loss=0.0032]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  97%|█████████▋| 124/128 [00:09<00:00, 13.07epoch/s, loss=0.00326, prev_loss=0.00308]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|█████████▊| 126/128 [00:09<00:00, 13.04epoch/s, loss=0.00319, prev_loss=0.00326]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  98%|█████████▊| 126/128 [00:10<00:00, 13.04epoch/s, loss=0.00314, prev_loss=0.00319]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|██████████| 128/128 [00:10<00:00, 12.66epoch/s, loss=0.00312, prev_loss=0.00314]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|██████████| 201/201 [00:00<00:00, 11.2ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.02s seconds\n",
      "INFO:pykeen.triples.triples_factory:Stored TriplesFactory(num_entities=14, num_relations=55, num_triples=1592, inverse_triples=False, path=\"E:\\AnacondaPython\\envs\\Thesis\\Lib\\site-packages\\pykeen\\datasets\\nations\\train.txt\") to file:///C:/Users/tommy/OneDrive/Documenten/year%203/Bachelor%20AI%20Project/coding/training_triples\n",
      "INFO:pykeen.pipeline.api:Saved to directory: file:///C:/Users/tommy/OneDrive/Documenten/year%203/Bachelor%20AI%20Project/coding\n"
     ]
    }
   ],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.datasets import Nations\n",
    "from pykeen.models import TransE\n",
    "\n",
    "\n",
    "dataset = Nations()\n",
    "\n",
    "\n",
    "pipeline_result = pipeline(\n",
    "    dataset= dataset,\n",
    "    model= TransE,\n",
    "    training_loop='sLCWA',\n",
    "    negative_sampler='basic',\n",
    "    evaluator='RankBasedEvaluator',\n",
    "    training_kwargs=dict(num_epochs=128),\n",
    "    random_seed = 3757357109,\n",
    "    #result_tracker='mlflow',\n",
    "    #result_tracker_kwargs=dict(\n",
    "        #tracking_uri='http://localhost:5000',\n",
    "        #experiment_name='Training of TransE on Nations',\n",
    "    #),\n",
    "    device='cpu'\n",
    ")\n",
    "pipeline_result.save_to_directory(r'C:\\Users\\tommy\\OneDrive\\Documenten\\year 3\\Bachelor AI Project\\coding')\n",
    "\n",
    "#seed 3757357109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f224194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6d2258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"Negative sampling algorithm based on the work of of Bordes *et al.*.\"\"\"\n",
    "\n",
    "import math\n",
    "from typing import Collection, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from pykeen.constants import LABEL_HEAD, LABEL_TAIL, TARGET_TO_INDEX\n",
    "from pykeen.typing import Target\n",
    "\n",
    "__all__ = [\n",
    "    \"BasicNegativeSampler\",\n",
    "    \"random_replacement_\",\n",
    "]\n",
    "\n",
    "\n",
    "def random_replacement_(batch: torch.LongTensor, index: int, selection: slice, size: int, max_index: int) -> None:\n",
    "    \"\"\"\n",
    "    Replace a column of a batch of indices by random indices.\n",
    "\n",
    "    :param batch: shape: `(*batch_dims, d)`\n",
    "        the batch of indices\n",
    "    :param index:\n",
    "        the index (of the last axis) which to replace\n",
    "    :param selection:\n",
    "        a selection of the batch, e.g., a slice or a mask\n",
    "    :param size:\n",
    "        the size of the selection\n",
    "    :param max_index:\n",
    "        the maximum index value at the chosen position\n",
    "    \"\"\"\n",
    "    # At least make sure to not replace the triples by the original value\n",
    "    # To make sure we don't replace the {head, relation, tail} by the\n",
    "    # original value we shift all values greater or equal than the original value by one up\n",
    "    # for that reason we choose the random value from [0, num_{heads, relations, tails} -1]\n",
    "    replacement = torch.randint(\n",
    "        high=max_index - 1,\n",
    "        size=(size,),\n",
    "        device=batch.device,\n",
    "    )\n",
    "    replacement += (replacement >= batch[selection, index]).long()\n",
    "    batch[selection, index] = replacement\n",
    "\n",
    "class BasicNegativeSampler(NegativeSampler):\n",
    "    r\"\"\"A basic negative sampler.\n",
    "\n",
    "    This negative sampler that corrupts positive triples $(h,r,t) \\in \\mathcal{K}$ by replacing either $h$, $r$ or $t$\n",
    "    based on the chosen corruption scheme. The corruption scheme can contain $h$, $r$ and $t$ or any subset of these.\n",
    "\n",
    "    Steps:\n",
    "\n",
    "    1. Randomly (uniformly) determine whether $h$, $r$ or $t$ shall be corrupted for a positive triple\n",
    "       $(h,r,t) \\in \\mathcal{K}$.\n",
    "    2. Randomly (uniformly) sample an entity $e \\in \\mathcal{E}$ or relation $r' \\in \\mathcal{R}$ for selection to\n",
    "       corrupt the triple.\n",
    "\n",
    "       - If $h$ was selected before, the corrupted triple is $(e,r,t)$\n",
    "       - If $r$ was selected before, the corrupted triple is $(h,r',t)$\n",
    "       - If $t$ was selected before, the corrupted triple is $(h,r,e)$\n",
    "    3. If ``filtered`` is set to ``True``, all proposed corrupted triples that also exist as\n",
    "       actual positive triples $(h,r,t) \\in \\mathcal{K}$ will be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        corruption_scheme: Optional[Collection[Target]] = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the basic negative sampler with the given entities.\n",
    "\n",
    "        :param corruption_scheme:\n",
    "            What sides ('h', 'r', 't') should be corrupted. Defaults to head and tail ('h', 't').\n",
    "        :param kwargs:\n",
    "            Additional keyword based arguments passed to :class:`pykeen.sampling.NegativeSampler`.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.corruption_scheme = corruption_scheme or (LABEL_HEAD, LABEL_TAIL)\n",
    "        # Set the indices\n",
    "        self._corruption_indices = [TARGET_TO_INDEX[side] for side in self.corruption_scheme]\n",
    "        \n",
    "\n",
    "    def corrupt_batch(self, positive_batch: torch.LongTensor) -> torch.LongTensor:  # noqa: D102\n",
    "        batch_shape = positive_batch.shape[:-1]\n",
    "\n",
    "        # clone positive batch for corruption (.repeat_interleave creates a copy)\n",
    "        negative_batch = positive_batch.view(-1, 3).repeat_interleave(self.num_negs_per_pos, dim=0)\n",
    "\n",
    "        # Bind the total number of negatives to sample in this batch\n",
    "        total_num_negatives = negative_batch.shape[0]\n",
    "\n",
    "        # Equally corrupt all sides\n",
    "        split_idx = int(math.ceil(total_num_negatives / len(self._corruption_indices)))\n",
    "\n",
    "        # Do not detach, as no gradients should flow into the indices.\n",
    "        for index, start in zip(self._corruption_indices, range(0, total_num_negatives, split_idx)):\n",
    "            stop = min(start + split_idx, total_num_negatives)\n",
    "            random_replacement_(\n",
    "                batch=negative_batch,\n",
    "                index=index,\n",
    "                selection=slice(start, stop),\n",
    "                size=stop - start,\n",
    "                max_index=self.num_relations if index == 1 else self.num_entities,\n",
    "            )\n",
    "\n",
    "        return negative_batch.view(*batch_shape, self.num_negs_per_pos, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd70f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Collection, Optional, Tuple\n",
    "import math\n",
    "from typing import Collection, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from pykeen.constants import LABEL_HEAD, LABEL_TAIL, TARGET_TO_INDEX\n",
    "from pykeen.typing import Target\n",
    "\n",
    "class MyNegativeSampler(NegativeSampler):\n",
    "    r\"\"\"A basic negative sampler.\n",
    "\n",
    "    This negative sampler that corrupts positive triples $(h,r,t) \\in \\mathcal{K}$ by replacing either $h$, $r$ or $t$\n",
    "    based on the chosen corruption scheme. The corruption scheme can contain $h$, $r$ and $t$ or any subset of these.\n",
    "\n",
    "    Steps:\n",
    "\n",
    "    1. Randomly (uniformly) determine whether $h$, $r$ or $t$ shall be corrupted for a positive triple\n",
    "       $(h,r,t) \\in \\mathcal{K}$.\n",
    "    2. Randomly (uniformly) sample an entity $e \\in \\mathcal{E}$ or relation $r' \\in \\mathcal{R}$ for selection to\n",
    "       corrupt the triple.\n",
    "\n",
    "       - If $h$ was selected before, the corrupted triple is $(e,r,t)$\n",
    "       - If $r$ was selected before, the corrupted triple is $(h,r',t)$\n",
    "       - If $t$ was selected before, the corrupted triple is $(h,r,e)$\n",
    "    3. If ``filtered`` is set to ``True``, all proposed corrupted triples that also exist as\n",
    "       actual positive triples $(h,r,t) \\in \\mathcal{K}$ will be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        triple: torch.Tensor,\n",
    "        corruption_scheme: Optional[Collection[Target]] = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the basic negative sampler with the given entities.\n",
    "\n",
    "        :param corruption_scheme:\n",
    "            What sides ('h', 'r', 't') should be corrupted. Defaults to head and tail ('h', 't').\n",
    "        :param kwargs:\n",
    "            Additional keyword based arguments passed to :class:`pykeen.sampling.NegativeSampler`.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.corruption_scheme = corruption_scheme or (LABEL_HEAD, LABEL_TAIL)\n",
    "        # Set the indices\n",
    "        self._corruption_indices = [TARGET_TO_INDEX[side] for side in self.corruption_scheme]\n",
    "        self.triple=triple\n",
    "        \n",
    "\n",
    "    def corrupt_batch(self, positive_batch: torch.LongTensor) -> torch.LongTensor:  # noqa: D102\n",
    "        batch_shape = positive_batch.shape[:-1]\n",
    "\n",
    "        # clone positive batch for corruption (.repeat_interleave creates a copy)\n",
    "        negative_batch = positive_batch.view(-1, 3).repeat_interleave(self.num_negs_per_pos, dim=0)\n",
    "\n",
    "        # Bind the total number of negatives to sample in this batch\n",
    "        total_num_negatives = negative_batch.shape[0]\n",
    "\n",
    "        # Equally corrupt all sides\n",
    "        split_idx = int(math.ceil(total_num_negatives / len(self._corruption_indices)))\n",
    "\n",
    "        # Do not detach, as no gradients should flow into the indices.\n",
    "        for index, start in zip(self._corruption_indices, range(0, total_num_negatives, split_idx)):\n",
    "            stop = min(start + split_idx, total_num_negatives)\n",
    "            random_replacement_(\n",
    "                batch=negative_batch,\n",
    "                index=index,\n",
    "                selection=slice(start, stop),\n",
    "                size=stop - start,\n",
    "                max_index=self.num_relations if index == 1 else self.num_entities,\n",
    "            )\n",
    "\n",
    "        \n",
    "        \n",
    "        negative_batch_array = negative_batch.view(*batch_shape, self.num_negs_per_pos, 3).numpy()\n",
    "        first_half = negative_batch_array[:int((len(negative_batch_array)/2))]\n",
    "        second_half = negative_batch_array[int((len(negative_batch_array)/2)):]\n",
    "        \n",
    "        second_half_with_triple = []\n",
    "        \n",
    "        for i in range(int((len(negative_batch_array)/2))):\n",
    "            second_half_with_triple.append([self.triple] )\n",
    "        \n",
    "        second_half_with_triple_array = np.array(second_half_with_triple)\n",
    "        first_half_array = np.array(first_half)\n",
    "        \n",
    "        test_list1 = np.ndarray.tolist(first_half_array)\n",
    "        test_list2 = np.ndarray.tolist(second_half_with_triple_array)\n",
    "        \n",
    "        negative_batch_with_triple_list = test_list1 + test_list2\n",
    "        \n",
    "        negative_batch_with_triple_array = np.array(negative_batch_with_triple_list)\n",
    "        \n",
    "        pt_tensor_from_list = torch.Tensor(negative_batch_with_triple_array)\n",
    "        typecst = pt_tensor_from_list.type(torch.int64)\n",
    "        \n",
    "        #return negative_batch.view(*batch_shape, self.num_negs_per_pos, 3)\n",
    "        return  typecst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd6ed0",
   "metadata": {},
   "source": [
    "# need a way to insert the triple into the class, then replace bottom half of batch with that triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2c9726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[13,  3,  2]],\n",
      "\n",
      "        [[ 9,  3,  3]],\n",
      "\n",
      "        [[ 6,  3, 10]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 8, 46,  7]],\n",
      "\n",
      "        [[ 8, 46,  7]],\n",
      "\n",
      "        [[ 8, 46,  7]]])\n",
      "[8, 46, 7]\n"
     ]
    }
   ],
   "source": [
    "mapped_triples = pipeline_result.training.mapped_triples\n",
    "triples_factory = pipeline_result.training\n",
    "triple=([8, 46, 7])\n",
    "sampler = MyNegativeSampler(mapped_triples=mapped_triples, triple=triple)\n",
    "\n",
    "print(sampler.corrupt_batch(mapped_triples))\n",
    "print(sampler.triple)\n",
    "#print(len(sampler.corrupt_batch(mapped_triples)))\n",
    "\n",
    "#print(sampler.corrupt_batch(mapped_triples))\n",
    "#sampler = BasicNegativeSampler(mapped_triples=mapped_triples_training_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "264e9820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.pipeline.api:Using device: cpu\n",
      "Training epochs on cpu:   0%|          | 0/4 [00:00<?, ?epoch/s]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:   0%|          | 0/4 [00:00<?, ?epoch/s, loss=0.00788, prev_loss=nan]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████     | 2/4 [00:00<00:00, 11.97epoch/s, loss=0.00571, prev_loss=0.00788]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu:  50%|█████     | 2/4 [00:00<00:00, 11.97epoch/s, loss=0.00409, prev_loss=0.00571]\n",
      "Training batches on cpu:   0%|          | 0/7 [00:00<?, ?batch/s]\u001b[A\n",
      "Training epochs on cpu: 100%|██████████| 4/4 [00:00<00:00, 11.97epoch/s, loss=0.00339, prev_loss=0.00409]\n",
      "INFO:pykeen.evaluation.evaluator:Currently automatic memory optimization only supports GPUs, but you're using a CPU. Therefore, the batch_size will be set to the default value.\n",
      "INFO:pykeen.evaluation.evaluator:No evaluation batch_size provided. Setting batch_size to '32'.\n",
      "Evaluating on cpu: 100%|██████████| 201/201 [00:00<00:00, 10.0ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.03s seconds\n"
     ]
    }
   ],
   "source": [
    "pipeline_result_false = pipeline(\n",
    "    dataset= dataset,\n",
    "    model= TransE,\n",
    "    loss= 'MarginRankingLoss',\n",
    "    loss_kwargs = dict(\n",
    "    reduction=\"mean\"),\n",
    "    training_loop='sLCWA',\n",
    "    negative_sampler=MyNegativeSampler,\n",
    "    negative_sampler_kwargs = dict(triple=triple), \n",
    "    evaluator='RankBasedEvaluator',\n",
    "    training_kwargs=dict(num_epochs=4),\n",
    "    random_seed = 3757357109,\n",
    "    #result_tracker='mlflow',\n",
    "    #result_tracker_kwargs=dict(\n",
    "        #tracking_uri='http://localhost:5000',\n",
    "        #experiment_name='Training of TransE on Nations',\n",
    "    #),\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c515b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a batch of new triples:\n",
    "from typing import Collection, Optional, Tuple\n",
    "import math\n",
    "from typing import Collection, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from pykeen.constants import LABEL_HEAD, LABEL_TAIL, TARGET_TO_INDEX\n",
    "from pykeen.typing import Target\n",
    "\n",
    "\n",
    "class MyNegativeSampler(NegativeSampler):\n",
    "    \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        triples_factory: TriplesFactory,\n",
    "        triple: torch.Tensor,\n",
    "        num_negs_per_pos: Optional[int] = None,\n",
    "        filtered: bool = False,\n",
    "        corruption_scheme: Optional[Collection[str]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the negative sampler with the given entities.\n",
    "\n",
    "        :param triples_factory: The factory holding the triples to sample from\n",
    "        :param num_negs_per_pos: Number of negative samples to make per positive triple. Defaults to 1.\n",
    "        :param filtered: Whether proposed corrupted triples that are in the training data should be filtered.\n",
    "            Defaults to False. See explanation in :func:`filter_negative_triples` for why this is\n",
    "            a reasonable default.\n",
    "        :param corruption_scheme: What sides ('h', 'r', 't') should be corrupted. Defaults to head and tail ('h', 't').\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            triples_factory=triples_factory,\n",
    "            num_negs_per_pos=num_negs_per_pos,\n",
    "            filtered=filtered,\n",
    "            corruption_scheme=corruption_scheme,\n",
    "        )\n",
    "        self.triple = triple\n",
    "     \n",
    "\n",
    "    def sample(self, positive_batch: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.Tensor]]:\n",
    "        negative_samples, mask = super().sample(positive_batch)\n",
    "        negative_samples[negative_samples.dim(0)/2, :, :] = self.triple.unsqueeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b843924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"Basic structure for a negative sampler.\"\"\"\n",
    "\n",
    "from abc import abstractmethod\n",
    "from typing import Any, ClassVar, Mapping, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from class_resolver import HintOrType, normalize_string\n",
    "from torch import nn\n",
    "\n",
    "from pykeen.sampling.filtering import Filterer, filterer_resolver\n",
    "from pykeen.typing import MappedTriples\n",
    "\n",
    "__all__ = [\n",
    "    \"NegativeSampler\",\n",
    "]\n",
    "\n",
    "\n",
    "class NegativeSampler(nn.Module):\n",
    "    \"\"\"A negative sampler.\"\"\"\n",
    "\n",
    "    #: The default strategy for optimizing the negative sampler's hyper-parameters\n",
    "    hpo_default: ClassVar[Mapping[str, Mapping[str, Any]]] = dict(\n",
    "        num_negs_per_pos=dict(type=int, low=1, high=100, log=True),\n",
    "    )\n",
    "\n",
    "    #: A filterer for negative batches\n",
    "    filterer: Optional[Filterer]\n",
    "\n",
    "    num_entities: int\n",
    "    num_relations: int\n",
    "    num_negs_per_pos: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        mapped_triples: MappedTriples,\n",
    "        num_entities: Optional[int] = None,\n",
    "        num_relations: Optional[int] = None,\n",
    "        num_negs_per_pos: Optional[int] = None,\n",
    "        filtered: bool = False,\n",
    "        filterer: HintOrType[Filterer] = None,\n",
    "        filterer_kwargs: Optional[Mapping[str, Any]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the negative sampler with the given entities.\n",
    "\n",
    "        :param mapped_triples:\n",
    "            the positive training triples\n",
    "        :param num_entities:\n",
    "            the number of entities. If None, will be inferred from the triples.\n",
    "        :param num_relations:\n",
    "            the number of relations. If None, will be inferred from the triples.\n",
    "        :param num_negs_per_pos:\n",
    "            number of negative samples to make per positive triple. Defaults to 1.\n",
    "        :param filtered: Whether proposed corrupted triples that are in the training data should be filtered.\n",
    "            Defaults to False. See explanation in :func:`filter_negative_triples` for why this is\n",
    "            a reasonable default.\n",
    "        :param filterer: If filtered is set to True, this can be used to choose which filter module from\n",
    "            :mod:`pykeen.sampling.filtering` is used.\n",
    "        :param filterer_kwargs:\n",
    "            Additional keyword-based arguments passed to the filterer upon construction.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_entities = num_entities or mapped_triples[:, [0, 2]].max().item() + 1\n",
    "        self.num_relations = num_relations or mapped_triples[:, 1].max().item() + 1\n",
    "        self.num_negs_per_pos = num_negs_per_pos if num_negs_per_pos is not None else 1\n",
    "        self.filterer = (\n",
    "            filterer_resolver.make(\n",
    "                filterer,\n",
    "                pos_kwargs=filterer_kwargs,\n",
    "                mapped_triples=mapped_triples,\n",
    "            )\n",
    "            if filterer is not None or filtered\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def get_normalized_name(cls) -> str:\n",
    "        \"\"\"Get the normalized name of the negative sampler.\"\"\"\n",
    "        return normalize_string(cls.__name__, suffix=NegativeSampler.__name__)\n",
    "\n",
    "    def sample(self, positive_batch: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.BoolTensor]]:\n",
    "        \"\"\"\n",
    "        Generate negative samples from the positive batch.\n",
    "\n",
    "        :param positive_batch: shape: (batch_size, 3)\n",
    "            The positive triples.\n",
    "\n",
    "        :return:\n",
    "            A pair `(negative_batch, filter_mask)` where\n",
    "\n",
    "            1. `negative_batch`: shape: (batch_size, num_negatives, 3)\n",
    "               The negative batch. ``negative_batch[i, :, :]`` contains the negative examples generated from\n",
    "               ``positive_batch[i, :]``.\n",
    "            2. filter_mask: shape: (batch_size, num_negatives)\n",
    "               An optional filter mask. True where negative samples are valid.\n",
    "        \"\"\"\n",
    "        # create unfiltered negative batch by corruption\n",
    "        negative_batch = self.corrupt_batch(positive_batch=positive_batch)\n",
    "\n",
    "        if self.filterer is None:\n",
    "            return negative_batch, None\n",
    "\n",
    "        # If filtering is activated, all negative triples that are positive in the training dataset will be removed\n",
    "        return negative_batch, self.filterer(negative_batch=negative_batch)\n",
    "\n",
    "    @abstractmethod\n",
    "    def corrupt_batch(self, positive_batch: torch.LongTensor) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Generate negative samples from the positive batch without application of any filter.\n",
    "\n",
    "        :param positive_batch: shape: `(*batch_dims, 3)`\n",
    "            The positive triples.\n",
    "\n",
    "        :return: shape: `(*batch_dims, num_negs_per_pos, 3)`\n",
    "            The negative triples. ``result[*bi, :, :]`` contains the negative examples generated from\n",
    "            ``positive_batch[*bi, :]``.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c966aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
